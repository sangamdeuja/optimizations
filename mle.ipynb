{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: What is a likelihood function? Also add a formula and explain what it means?\n",
    "\n",
    "\n",
    "The likelihood function which is also known as likelihood expresses the values on y axis against a set of given observations. It is equal to joint probability distribution of the random samples. \n",
    "\n",
    "In a simpliest way, likelihood function can be defined as a function which expresses the value of likelihood against a value of an unknown parameter. \n",
    "\n",
    "Let $X_{1}, X_{2}, \\ldots, X_{n}$ have a joint density function $f\\left(X_{1}, X_{2}, \\dots, X_{n} | \\theta\\right)$. Given $X_{1}=x_{1}, X_{2}= x_{2}, \\ldots, X_{n}=x_{n}$ is observed, the function is $\\theta$ defined by:\n",
    "\n",
    "$$L(\\theta)=L\\left(\\theta | x_{1}, x_{2}, \\ldots, x_{n}\\right)=f\\left(x_{1}, x_{2}, \\ldots, x_{n} | \\theta\\right)$$\n",
    "\n",
    "However,\n",
    "\n",
    "1. The likelihood function is not a probability density function.\n",
    "\n",
    "2. It is an important component of both frequentist and bayesian analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: What is Maximum Likelihood Estimation (MLE) ? Can you give an example? \n",
    "## Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "MLE is method of estimating the unknown parameter $\\theta$ of a model, given observed data. It estimates the model parameter by finding the parameter value that maximizes the likelihood function. The parameter estimate is called maximum likelihood estimate $\\hat{\\theta}_{M L E}$. \n",
    "\n",
    "\n",
    "\n",
    "Let $X_{1}, X_{2}, \\ldots, X_{n}$ be a random sample from a distribution that depends on one or more unknown parameters $\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{m}$ with probability density (or mass) function $f\\left(x_{i} ; \\theta_{1}, \\theta_{2}, \\ldots, \\theta_{m}\\right)$. Suppose that $\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{m}\\right)$ is restricted to a given parameter space $Ω$. Then:\n",
    "\n",
    "(1) When regarded as a function of $\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{m}$, the joint probability density (or mass) function of $X_{1}, X_{2}, \\ldots, X_{n}$:\n",
    "\n",
    "$$L\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{m}\\right)=\\prod_{i=1}^{n} f\\left(x_{i} ; \\theta_{1}, \\theta_{2}, \\ldots, \\theta_{m}\\right)$$\n",
    "\n",
    "\n",
    "$\\left(\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{m}\\right) \\text { in } \\Omega\\right)$ is called the likelihood function.\n",
    "\n",
    "\n",
    "$\\begin{array}{l}{\\text { (2) If: }} \\\\ {\\qquad\\left[u_{1}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right), u_{2}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right), \\ldots, u_{m}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\\right]}\\end{array}$\n",
    "\n",
    "is the m-tuple that maximizes the likelihood function, then:\n",
    "\n",
    "$$\\hat{\\theta}_{i}=u_{i}\\left(X_{1}, X_{2}, \\ldots, X_{n}\\right)$$\n",
    "is the maximum likelihood estimator of $\\theta_{i}, \\text { for } i=1,2, \\ldots, m$\n",
    "\n",
    "\n",
    "3) The corresponding observed values of the statistics in (2), namely:\n",
    "\n",
    "$$\\left[u_{1}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right), u_{2}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right), \\ldots, u_{m}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\\right]$$\n",
    "\n",
    "are called the maximum likelihood estimates of $\\theta_{i}, \\text { for } i=1,2, \\ldots, m.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:\n",
    "\n",
    "Suppose the weights of randomly selected American female college students are normally distributed with unknown mean $μ$ and standard deviation $σ$. A random sample of 10 American female college students yielded the following weights (in pounds):\n",
    "\n",
    "115   122   130   127   149   160   152   138  149   180    \n",
    "\n",
    "Based on the definitions given above, identify the likelihood function and the maximum likelihood estimator of $μ$, the mean weight of all American female college students. Using the given sample, find a maximum likelihood estimate of $μ$ as well.\n",
    "\n",
    "\n",
    "### Solution:\n",
    "The probability density function of $X_{i}$ is:\n",
    "$$f\\left(x_{i} ; \\mu, \\sigma^{2}\\right)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left[-\\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}\\right]$$\n",
    "\n",
    "for −∞ < x < ∞. The parameter space is Ω = {(μ, σ): −∞ < μ < ∞ and 0 < σ < ∞}. Therefore, (you might want to convince yourself that) the likelihood function is:\n",
    "$$L(\\mu, \\sigma)=\\sigma^{-n}(2 \\pi)^{-n / 2} \\exp \\left[-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}\\right]$$\n",
    "\n",
    "for −∞ < μ < ∞ and 0 < σ < ∞. It can be shown (we'll do so in the next example!), upon maximizing the likelihood function with respect to μ, that the maximum likelihood estimator of μ is:\n",
    "\n",
    "$$\\hat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i}=\\overline{X}$$\n",
    "\n",
    "Based on the given sample, a maximum likelihood estimate of μ is:\n",
    "\n",
    "$$\\hat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}=\\frac{1}{10}(115+\\cdots+180)=142.2$$\n",
    "\n",
    "pounds. Note that the only difference between the formulas for the maximum likelihood estimator and the maximum likelihood estimate is that:\n",
    "\n",
    "* the estimator is defined using capital letters (to denote that its value is random), and\n",
    "\n",
    "* the estimate is defined using lowercase letters (to denote that its value is fixed and based on an obtained sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q : How is linear regression related to Pytorch and gradient descent? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression refers to the linear relationships between a dependent variable and one or more independent variables. If $Y$ is a dependent variable and $X_{1}$ and $X_{2}$ are dependent variables then we can have the following equation to establish the linear relationship.\n",
    "\n",
    "$Y$ = a$X_{1}$ + b$X_{2}$ + $C$\n",
    "\n",
    "Where,\n",
    "\n",
    "$Y$ = Dependent variable\n",
    "\n",
    "a = coefficient/slope of $X_{1}$ variable\n",
    "\n",
    "b = coefficient/slope of $X_{2}$ variable\n",
    "\n",
    "$X_{1}$ = independent variable\n",
    "\n",
    "$X_{2}$ = independent variable\n",
    "\n",
    "$C$ = Intercept\n",
    "\n",
    "The objective of the data analysis in term of linear regression to find out the best fitted line which can be used as the reference for test data set for the accuracy determination. However, the fitted line relies on the values of intercpt ($C$) and slopes ($a$,$b$) to find a gradient descent where the error is minimum. In the other way we can say, it is to reduce the loss function. Pytorch is one of the built in libraries which is being used to find the loss function with a minimal number of codings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Write out MSE loss for linear regression. Could we also use this loss for classification?\n",
    "\n",
    "MSE(mean square error)/quadratic/L2 loss function is the most commonly used function to predict the model's predicted output to the actual output. Mathematically this can be expressed as\n",
    "\\begin{equation*}\n",
    "MSE =  \\frac{\\sum_{i=1}^n (Y_i-y_i)^2)}{n}\n",
    "\\end{equation*}\n",
    "where Y is the actual output and y is the predicted output.\n",
    "Some of the common loss functions used for ML are given here [1](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0) , [2](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mse-l2) and [3](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\n",
    "We cannot use MSE for classification problem because classification problems are categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Write out the Maximum likelihood Estimation for linear regression. How is this related to the MSE loss for linear regression derived in the last point? Derive the relation between them. \n",
    "\n",
    " We assume the noise is gaussian and can be represented as:\n",
    "$\\epsilon \\sim N\\left(0, \\sigma^{2}\\right)$\n",
    "The linear equation can be written as:\n",
    "<p>\n",
    "$y=\\theta_{1} x+\\theta_{0}+\\epsilon$</p>\n",
    "To apply maximum likelihood, we first need to derive the likelihood function. First, let's rewrite our model from above as a single conditional distribution given x:\n",
    "$y \\sim N\\left(\\theta_{1} x+\\theta_{0}, \\sigma^{2}\\right)$\n",
    "the equation of a Gaussian distribution's probability density function, with our linear equation in place of the mean( Because mean is the value of function y):\n",
    "$f\\left(y | x ; \\theta_{0}, \\theta_{1}, \\sigma^{2}\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\frac{-\\left(y-(\\theta_{1} x+\\theta_{0})\\right)^{2}}{2 \\sigma^{2}}}$\n",
    "<p>Each point is independent and identically distributed (iid), so we can write the likelihood function with respect to all of our observed points as the product of each individual probability density.</p>\n",
    "$L_{X}\\left(\\theta_{0}, \\theta_{1}, \\sigma^{2}\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\prod_{(x, y) \\in X} e^{\\frac{-\\left(y-(\\theta_{1} x+\\theta_{0})\\right)^{2}}{2 \\sigma^{2}}}$\n",
    "<p> To make our equation simpler, let's take the log of our likelihood.</p>\n",
    "$l_{X}\\left(\\theta_{0}, \\theta_{1}, \\sigma^{2}\\right)=\\log \\left[\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\prod_{(x, y) \\in X} e^{\\frac{-\\left(y-\\left(\\theta_{1} x+\\theta_{0}\\right)^{2}\\right.}{2 \\sigma^{2}} )^{2}}\\right]$\n",
    "$=\\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)+\\sum_{(x, y) \\in X} \\log \\left(e^{\\frac{-\\left(y-(\\theta_{1} x+\\theta_{0})\\right)^{2}}{2 \\sigma^{2}}}\\right)$\n",
    "$=\\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)+\\sum_{(x, y) \\in X} \\frac{-\\left(y-\\left(\\theta_{1} x+\\theta_{0}\\right)\\right)^{2}}{2 \\sigma^{2}}$\n",
    "$=\\log (1)-\\log \\left(\\sqrt{2 \\pi \\sigma^{2}}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{(x, y) \\in X}\\left[y-\\left(\\theta_{1} x+\\theta_{0}\\right)\\right]^{2}$\n",
    "$=-\\log \\left(\\sqrt{2 \\pi \\sigma^{2}}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{(x, y) \\in X}\\left[y-\\left(\\theta_{1} x+\\theta_{0}\\right)\\right]^{2}$\n",
    "<p>maximizing a number is the same thing as minimizing the negative of the number. So instead of maximizing the likelihood, let's minimize the negative log-likelihood:</p>\n",
    "$-l_{X}\\left(\\theta_{0}, \\theta_{1}, \\sigma^{2}\\right)=\\log \\left(\\sqrt{2 \\pi \\sigma^{2}}\\right)+\\frac{1}{2 \\sigma^{2}} \\sum(y-\\hat{y})^{2}$\n",
    "The constant term can be neglected and the minimizing the negative log likelihood function(or maximizing log likelihood function) turns out to be minimizing mean square error.\n",
    "[Reference](https://towardsdatascience.com/linear-regression-91eeae7d6a2e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Write out the likelihood function for linear classification. What is the drawback of using MSE loss here?\n",
    "\n",
    "the logit model the output variable  $y_{i}$ is a Bernoulli random variable (it can take only two values, either 1 or 0)\n",
    "<p> $\\mathrm{P}\\left(y_{i}=1 | x_{i}\\right)=S\\left(x_{i} \\beta\\right)$</p>\n",
    "where\n",
    "$S(t)=\\frac{1}{1+\\exp (-t)}$ is the logistic function,  $x_{i}$ is a  $1xK$ vector of inputs and  $\\beta $ is a  Kx1 vector of coefficients.\n",
    "Furthermore,\n",
    "the likelihood of the entire sample is equal to the product of the likelihoods of the single observations:\n",
    "$L\\left(\\beta ; y_{i}, x_{i}\\right)=\\left[S\\left(x_{i} \\beta\\right)\\right]^{y_{i}}\\left[1-S\\left(x_{i} \\beta\\right)\\right]^{1-y_{i}}$\n",
    "$\\begin{array} l(\\beta ; y, X) &=\\ln (L(\\beta ; y, X)) \\\\ &=\\ln \\left(\\prod_{i=1}^{N}\\left[S\\left(x_{i} \\beta\\right)\\right]^{y_{i}}\\left[1-S\\left(x_{i} \\beta\\right)\\right]^{1-y_{i}}\\right) \\\\ &=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(S\\left(x_{i} \\beta\\right)\\right)+\\left(1-y_{i}\\right) \\ln \\left(1-S\\left(x_{i} \\beta\\right)\\right)\\right]\n",
    "\\end{array}$\n",
    "$\n",
    "\\begin{array}{l}{=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+\\left(1-y_{i}\\right) \\ln \\left(1-\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right]} \\\\ {=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+\\left(1-y_{i}\\right) \\ln \\left(\\frac{1+\\exp \\left(-x_{i} \\beta\\right)-1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right]} \\\\ {=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+\\left(1-y_{i}\\right) \\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right]}\\end{array}$\n",
    "$\n",
    "\\begin{array}{l}{=\\sum_{i=1}^{N}\\left[\\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+y_{i}\\left(\\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)-\\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right)\\right]} \\\\ {=\\sum_{i=1}^{N}\\left[\\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)} \\frac{\\exp \\left(x_{i} \\beta\\right)}{\\exp \\left(x_{i} \\beta\\right)}\\right)+y_{i}\\left(\\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)} \\frac{1+\\exp \\left(-x_{i} \\beta\\right)}{\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right\\}\\right]} \\\\ {\\quad=\\sum_{i=1}^{N}\\left[\\ln \\left(\\frac{1}{1+\\exp \\left(x_{i} \\beta\\right)}\\right)+y_{i}\\left(\\ln \\left(\\frac{1}{\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right)\\right]}\\end{array}\n",
    "$\n",
    "$\n",
    "\\begin{array}{l}{=\\sum_{i=1}^{N}\\left[\\ln (1)-\\ln \\left(1+\\exp \\left(x_{i} \\beta\\right)\\right)+y_{i}\\left(\\ln (1)-\\ln \\left(\\exp \\left(-x_{i} \\beta\\right)\\right)\\right]\\right.} \\\\ {=\\sum_{i=1}^{N}\\left[-\\ln \\left(1+\\exp \\left(x_{i} \\beta\\right)\\right)+y_{i} x_{i} \\beta\\right]}\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q. Can gradient descent be used to find the parameters for linear regression? What about linear classification? Why?  \n",
    "\n",
    "Yes, Gradient Descent can be used to find the parameters for linear regression. In linear classicification, the inputs are contineous but the outputs are binary (0,1). According to the linear classification function, the hypothetical function of linear classification is sigmoid where the gradient descent can be used. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: What are normal equations? Is it the same as least squares? Explain.\n",
    "Normal equations are  technique for computing coefficients for Multivariate Linear Regression.\n",
    "This problem is also called OLS Regression, and Normal Equation is an approach of solving it\n",
    "It finds the regression coefficients analytically.\n",
    "It's an one-step learning algorithm as opposed to Gradient Descent which is iterative process of finding the regression coefficients\n",
    "This approach is an effective and a time-saving option when are working with a dataset with small features.<br>\n",
    "__Normal Equation is a follows :__\n",
    "$$ \\theta = ({X}^T{X})^{-1}.({X}^T{y}) $$\n",
    "In the above equation,<br>\n",
    "$θ$ : hypothesis parameters that define it the best.<br>\n",
    "$X$ : Input feature value of each instance.<br>\n",
    "$Y$ : Output value of each instance.<br>\n",
    "__Maths Behind the equation –__\n",
    "Given the hypothesis function <br>\n",
    "$$ h(\\theta) = \\theta_0{x_0} + \\theta_1{x_1}+...... + \\theta_n{x_n} $$\n",
    "where,<br>\n",
    "$n$ : the no. of features in the data set.<br>\n",
    "${x_0}$ : 1 (for vector multiplication)<br>\n",
    "Notice that this is dot product between θ and x values. So for the convenience to solve we can write it as :<br>\n",
    "$$ h(\\theta) = \\theta ^ T{x}$$\n",
    "The motive in Linear Regression is to minimize the cost function :<br>\n",
    "$$J(\\Theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} \\frac{1}{2} [h_{\\Theta}(x^{(i)}) - y^{(i)}]^{2} $$\n",
    "where,<br>\n",
    "$x_i$ : the input value of iih training example.<br>\n",
    "$m$ : no. of training instances<br>\n",
    "$n$ : no. of data-set features<br>\n",
    "$y_i$ : the expected result of ith instance<br>\n",
    "Let us representing cost function in a vector form<br>\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "h_\\theta ({x}^0) \\\\\n",
    "h_\\theta ({x}^1) \\\\\n",
    ".......\\\\\n",
    "h_\\theta ({x}^m) \\\\\n",
    "\\end{bmatrix}\n",
    "- \\begin{bmatrix}\n",
    "({y}^0) \\\\\n",
    "({y}^1) \\\\\n",
    ".......\\\\\n",
    "({y}^m) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br><br>we have ignored 1/2m here as it will not make any difference in the working. It was used for the mathematical convenience while calculation gradient descent. But it is no more needed here.<br>\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\theta ^ T ({x}^0) \\\\\n",
    "\\theta ^ T ({x}^1) \\\\\n",
    ".......\\\\\n",
    "\\theta ^ T ({x}^m) \\\\\n",
    "\\end{bmatrix} - y\n",
    "$$\n",
    "$\\theta_0 \\begin{pmatrix} 0   \\\\ {x_0} \\end{pmatrix}$ +\n",
    "$\\theta_1\n",
    "\\begin{pmatrix}\n",
    "0   \\\\\n",
    "{x_1}\n",
    "\\end{pmatrix}\n",
    "$\n",
    "${x}^i_j$ : value of ${j}^{ih}$ feature in ${i}^{ih}$ training example.\n",
    "This can further be reduced to  $X\\theta - y$<br>\n",
    "But each residual value is squared. We cannot simply square the above expression. As the square of a vector/matrix is not equal to the square of each of its values. So to get the squared value, multiply the vector/matrix with its transpose. So, the final equation derived is\n",
    "$$(X\\theta - y)^{T}(X\\theta - y)$$\n",
    "Therefore, the cost function is\n",
    "$$Cost = (X\\theta - y)^{T}(X\\theta - y) $$\n",
    "So, now getting the value of θ using derivative\n",
    "$$\\frac{\\partial J_{\\theta}}{\\partial {\\theta}} = \\frac{\\partial}{\\partial {\\theta}}{[(X{\\theta}- y)^T{(X{\\theta}- y)}]}$$\n",
    "$$ \\frac{\\partial J_{\\theta}}{\\partial {\\theta}} = 2X^TX\\theta - 2X^Ty$$\n",
    "$$ Cost^{'}(\\theta) = 0 $$\n",
    "$$2X^{T}X{\\theta} - 2X^Ty = 0$$\n",
    "$$2X^{T}X{\\theta} = 2X^Ty$$\n",
    "$$ (X^TX)^{-1}(X^TX){\\theta} = (X^TX)^{-1}.(X^Ty) $$\n",
    "$$\\theta = (X^TX)^{-1}.(X^Ty)$$\n",
    "So, this is the finally derived Normal Equation with θ giving the minimum cost value.\n",
    "[Reference](https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Alternative!\n",
    "\n",
    "Given a matrix equation\n",
    "\n",
    " A$x$=b, \n",
    "the normal equation is that which minimizes the sum of the square differences between the left and right sides:\n",
    "\n",
    " $A^(T)$A$x$=$A^(T)$b. \n",
    "It is called a normal equation because b-Ax is normal to the range of A.\n",
    "\n",
    "Here, $A^(T)$A is a normal matrix.\n",
    "\n",
    "Normal equations are used to solve the least square error. They are called the normal equations because they specify that the residual must be normal (orthogonal) to every vector in the span of A.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1170px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
